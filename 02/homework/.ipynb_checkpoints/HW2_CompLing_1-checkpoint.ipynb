{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0W3vlijxKLd"
   },
   "source": [
    "# HMM и Разметка частей речи\n",
    "\n",
    "Мы не будем реализовывать алгоритмы HMM с нуля, а воспользуемся библиотекой ``hmmlearn`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5791,
     "status": "ok",
     "timestamp": 1589740623144,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhQNy-ts-7hx9PAJ0YHgAIjwD0nfjAtMRdfDrCyQ=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "u_sxxN-m7LK2",
    "outputId": "6ff7f184-90f1-4c6d-e844-e962d95939d5"
   },
   "outputs": [],
   "source": [
    "# !pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddTB2_igH0vg"
   },
   "source": [
    "## Строим POS-tagger для английского языка\n",
    "- для обучения мы воспользуемся размеченными данными [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), который можно скачать прямо из библитеки nltk\n",
    "- мы будем использовать universal_tagset из nltk (это не таги Universal Dependencies, о которых мы говорили на лекции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1589746367152,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhQNy-ts-7hx9PAJ0YHgAIjwD0nfjAtMRdfDrCyQ=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "luRAKwiNP1pn",
    "outputId": "384b68e8-207a-4908-d6ca-bf4828ec2591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/antonina.goryacheva/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/antonina.goryacheva/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtsiB6X2p7-z"
   },
   "source": [
    "### Вам нужно выполнить следующие шаги, чтобы построить POS-tagger\n",
    "\n",
    "**Вопросы:**\n",
    "\n",
    "1. Возьмите размеченные предложения из  ``nltk.corpus.brown.tagged_sents(tagset='universal')``  \n",
    "\n",
    "    - Используйте ``sklearn.model_selection.train_test_split`` чтобы разделить корпус на  80% training data и 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "X_train, X_test = train_test_split(data, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57340, 45872, 11468)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Создайте  array ``pos_tags``  содержащий все уникальные POS таги, которые есть в трейн корпусе. Сколько уникальных тагов у вас получилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число уникальных POS тагов:  12\n"
     ]
    }
   ],
   "source": [
    "pos_tags = set([tag for text in X_train for (word, tag) in text])\n",
    "print('Число уникальных POS тагов: ', len(pos_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Найдите 5000 наиболее частов токенов (используйте collections.Counter). \n",
    "    - Перед подсчетом сделайте все буквы в словах маленькими (lowercase)\n",
    "    - Сохраните эти 5000 унаиболее частотных токенов в array ``vocab``. \n",
    "    -  Добавьте токен '[UNK]' в качестве первого элемента ``vocab`` для обозначения всех слов, не вошедших вчастотный словарь =  \"out of vocabulary\")\n",
    "    - Проверьте себя: первые 5 элементов ``vocab`` должны быть \\[\"\\[UNK\\]\", \"the\", \",\", \".\", \"of\", ...\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = Counter()\n",
    "for pair in X_train: word_dict.update([word.lower() for word, _ in pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', ',', '.', 'of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5000\n",
    "vocab = ['[UNK]'] + [word for word, _ in word_dict.most_common(N)]\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Используйте ``hmmlearn.hmm.MultinomialHMM``, чтобы создать модель``pos_model`` для разметки частей речи (POS-tagging). \n",
    "\n",
    "  * Установите ``pos_model.startprob_`` используя информацию о доле предложений, начинающихся с каждого из POS тагов из вашего списка уникальных тагов. Например, какая доля предложений начинается с глагола и т.д. \n",
    "     -  Подсказка: это должен быть лист длины ``len(pos_tags)``, сумма вероятностей бытьв начале предложения должна бытьравна 1.\n",
    "  \n",
    "  * Установите ``pos_model.transmat_`` - вероятность перехода от одного тага к другому на основе данных трейн корпуса.\n",
    "      - Подсказка: это должна быть матрица ``(len(pos_tags), len(pos_tags))``, сумма по каждой из строк матрицы должна быть равна 1.\n",
    "  \n",
    "  * Установите ``pos_model.emissionprob_`` - вероятность для каждого токена из ``vocab`` относится к какой-либо части речи. \n",
    "      - Убедитесь, что все токены состоят только из маленьких букв. \n",
    "      - Все токены не из ``vocab`` заменены на  \"\\[UNK\\]\". \n",
    "      - Подсказка: это должна быть матрица ``(len(pos_tags), len(vocab))`` , сумма по каждой из строк матрицы должна быть равна 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 0, 'NOUN': 1, 'PRT': 2, 'ADJ': 3, 'ADV': 4, 'PRON': 5, 'VERB': 6, 'DET': 7, 'NUM': 8, 'CONJ': 9, 'ADP': 10, '.': 11}\n"
     ]
    }
   ],
   "source": [
    "pos_idx = {tag: i for i, tag in enumerate(pos_tags)}\n",
    "vocab_idx = {word: i for i, word in enumerate(vocab)}\n",
    "print(pos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check sum:  1.0\n"
     ]
    }
   ],
   "source": [
    "# априоорные вероятности\n",
    "startprob = np.zeros(len(pos_tags))\n",
    "for text in X_train:\n",
    "    key = pos_idx[text[0][1]]\n",
    "    startprob[key] += 1\n",
    "\n",
    "n_texts = len(X_train)\n",
    "startprob = startprob / n_texts\n",
    "\n",
    "print('Check sum: ', sum(startprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица перехдов\n",
    "transmat =  np.zeros((len(pos_tags), len(pos_tags)))\n",
    "for text in X_train:\n",
    "    for i in range(len(text)-1):\n",
    "        key_1 = pos_idx[text[i][1]]\n",
    "        key_2 = pos_idx[text[i+1][1]]\n",
    "        transmat[key_1, key_2] += 1\n",
    "        \n",
    "row_sums = transmat.sum(axis=1)\n",
    "transmat_normed = transmat / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissionprob = np.zeros((len(pos_tags), len(vocab)))\n",
    "for text in X_train:\n",
    "    for word, tag in text:\n",
    "        word = word.lower()\n",
    "        key_1 = pos_idx[tag]\n",
    "        if word in vocab:\n",
    "            key_2 = vocab_idx[word]\n",
    "        else:\n",
    "            key_2 = vocab_idx[\"[UNK]\"]\n",
    "        emissionprob[key_1, key_2] += 1\n",
    "        \n",
    "row_sums = emissionprob.sum(axis=1)\n",
    "emissionprob_normed = emissionprob / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model = MultinomialHMM(n_components=len(pos_tags))\n",
    "\n",
    "pos_model.startprob_ = startprob\n",
    "pos_model.transmat_ = transmat_normed\n",
    "pos_model.emissionprob_ = emissionprob_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Напишите функцию``get_pos(sentence)``, которая возвращает наиболее вероятную последовательность тегов для некоторого предложения that (``sentence``) \n",
    "    - в этой функции используйте pos_model.decode(...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence, pos_model, vocab_idx, pos_idx):\n",
    "    reverse_pos_idx = {v:k for k,v in pos_idx.items()}\n",
    "    splited = sentence.split(' ')\n",
    "    tokens = [vocab_idx.get(x.lower(), 0) for x in splited]\n",
    "    logprob, seq = pos_model.decode(np.atleast_2d(tokens).T)\n",
    "    return [reverse_pos_idx[x] for x in seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Попробуйте применить вашу модель к нескольким предложениям (слова только измаленьких букв, без пунктуации).\n",
    "    - Обязательно попробуйте для предложений: \"this is a test\", \"saint petersburg is the second-largest city in russia.\", \"i know how to use hmm\". \n",
    "    - Прокомментируйте получившиеся результаты. Похоже на правду?\n",
    "    - Если появились идеи, почему модель ошибается, напишите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'VERB', 'DET', 'NOUN']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"this is a test\"\n",
    "get_pos(sentence, pos_model, vocab_idx, pos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"saint petersburg is the second-largest city in russia.\"\n",
    "get_pos(sentence, pos_model, vocab_idx, pos_idx)\n",
    "\n",
    "# saint petersburg = PROPN\n",
    "# russia = PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'ADV', 'PRT', 'VERB', 'VERB']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i know how to use hmm\"\n",
    "get_pos(sentence, pos_model, vocab_idx, pos_idx)\n",
    "\n",
    "# to use - инфинитив, а не 'PRT' +'VERB'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Бонус:** Оцените правильность этой модели на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i sighed , thinking that among other things , people here seemed to be those who would have to cut down if they earned less than $85,000 yearly ; ;\n",
      "true:  ['PRON', 'VERB', '.', 'VERB', 'ADP', 'ADP', 'ADJ', 'NOUN', '.', 'NOUN', 'ADV', 'VERB', 'PRT', 'VERB', 'DET', 'PRON', 'VERB', 'VERB', 'PRT', 'VERB', 'PRT', 'ADP', 'PRON', 'VERB', 'ADJ', 'ADP', 'NOUN', 'ADV', '.', '.']\n",
      "pred:  ['PRON', 'VERB', '.', 'VERB', 'ADP', 'ADP', 'ADJ', 'NOUN', '.', 'NOUN', 'ADV', 'VERB', 'PRT', 'VERB', 'DET', 'PRON', 'VERB', 'VERB', 'PRT', 'VERB', 'PRT', 'ADP', 'PRON', 'VERB', 'ADV', 'ADP', 'ADJ', 'NOUN', '.', '.']\n",
      "\n",
      "1 thus the films seen as they came in ( coordinated for the regular sections ) , were often out of context .\n",
      "true:  ['ADV', 'DET', 'NOUN', 'VERB', 'ADP', 'PRON', 'VERB', 'PRT', '.', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', '.', '.', 'VERB', 'ADV', 'PRT', 'ADP', 'NOUN', '.']\n",
      "pred:  ['ADV', 'DET', 'NOUN', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', '.', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', '.', 'VERB', 'ADV', 'PRT', 'ADP', 'NOUN', '.']\n",
      "\n",
      "2 opera lovers will be interested to learn that this church was the scene for the first act of tosca .\n",
      "true:  ['NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'PRT', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "pred:  ['NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'PRT', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "\n",
      "3 to prepare mustard cream , blend mustard with enough water to make a thin paste .\n",
      "true:  ['PRT', 'VERB', 'NOUN', 'NOUN', '.', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'VERB', 'DET', 'ADJ', 'NOUN', '.']\n",
      "pred:  ['PRT', 'VERB', 'ADJ', 'NOUN', '.', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'VERB', 'DET', 'ADJ', 'NOUN', '.']\n",
      "\n",
      "4 from the outset of his first term , he established himself as one of the guiding spirits of the house of delegates .\n",
      "true:  ['ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'PRON', 'VERB', 'PRON', 'ADP', 'NUM', 'ADP', 'DET', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "pred:  ['ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'PRON', 'VERB', 'PRON', 'ADP', 'NUM', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# просто посмотреть глазами\n",
    "for i, text in enumerate(X_test[:5]):\n",
    "    sentence = ' '.join([word.lower() for word, _ in text])\n",
    "    true_res = [x[1] for x in text]\n",
    "    model_res = get_pos(sentence, pos_model, vocab_idx, pos_idx)\n",
    "    print(i, sentence)\n",
    "    print('true: ', true_res)\n",
    "    print('pred: ', model_res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, text in enumerate(X_test):\n",
    "    sentence = ' '.join([word.lower() for word, _ in text])\n",
    "    true_res = [x[1] for x in text]\n",
    "    model_res = get_pos(sentence, pos_model, vocab_idx, pos_idx)\n",
    "    res = [1 if true_ == pred_ else 0 for true_, pred_ in zip(true_res, model_res)]\n",
    "    result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9292224195048822"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# доля верно угаданных частей речи среди всех\n",
    "np.mean([sum(r) / len(r) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34365190094175097"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# доля предложений, в которых угадано 100% верно\n",
    "tmp = [sum(r) / len(r) for r in result]\n",
    "sum([1 for x in tmp if x==1 ]) / len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение присылйте на почту  bdt-mf-ml-nlp-2020-q4@bigdatateam.org  \n",
    "\n",
    "В теме письма укажите: ``HW2:CompLing. ФИО``\n",
    "\n",
    "Пожалуйста, оставьте обратную связь о задании [по ссылке](http://rebrand.ly/mfnlp2020q4_feedback_hw02). Она (при желании) анонимна ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
