{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNB-StCcK_k0"
   },
   "source": [
    "# Семинар 1. Word embeddings\n",
    "На этом занятии мы познакомимся с несколькими моделями векторных представлений слов: обучим с нуля пару простых моделей, убедимся в том, что в пространстве word2vec векторные операции соответствуют смысловым изменениям, а также попробуем решить с их помощью прикладную задачу sentiment analysis. \n",
    "\n",
    "Для первой части воспользуемся набором неразмеченных текстов, часто используемым для моделирования языка — [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rPjqunep10cV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n",
      "unzip:  cannot find or open wikitext-2-v1, wikitext-2-v1.zip or wikitext-2-v1.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!wget -q -nc https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip -o wikitext-2-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cluw6DmTPNL"
   },
   "source": [
    "Для начала считаем все строки из входного файла (уже разбитого на токены) и приведём их к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oNnD1hQ2D0n"
   },
   "outputs": [],
   "source": [
    "with open('wikitext-2/wiki.train.tokens') as f:\n",
    "    lines = [line.strip().lower() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIaJLJiea3QY"
   },
   "source": [
    "Чтобы подготовить данные для обучения, сначала необходимо построить словарь — отображение из слова в его индекс и наоборот. Также нам впоследствии может пригодиться получение списка самых частых слов, поэтому стоит для каждого слова сохранять число его вхождений в корпус. Пользуясь классом [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), постройте счётчик вхождений каждого слова в датасет, а также словари word_to_ind и ind_to_word, отображающие слово в целочисленный индекс и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCWlHSD9IATV"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "\n",
    "for line in lines:\n",
    "    if line:\n",
    "        words = line.split()\n",
    "        vocab.update(words)\n",
    "\n",
    "word_to_ind = {word : i for i, word in enumerate(vocab)}\n",
    "ind_to_word = {i : word for word, i in word_to_ind.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7j9uYX_a-cM"
   },
   "source": [
    "Создайте матрицу попарной встречаемости слов cooc_matrix размера `len(vocab) x len(vocab)`, в ячейке i,j которой содержится информация о том, как часто  слова с индексами i и j находились в контексте друг друга. В качестве контекста используйте скользящее окно с центром в каждом слове предложения размера 5.\n",
    "\n",
    "Bonus tip: строго говоря, более разумно для построения word embeddings использовать матрицу Pointwise Mutual Information (PMI). При желании можно обратиться к [статье](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf) и реализовать подсчёт этой характеристики — последующие результаты должны стать лучше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifcMn-QvI_nn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cooc_matrix = np.zeros((len(vocab),len(vocab)),dtype=np.float32)\n",
    "\n",
    "CONTEXT_WINDOW_SIZE = 5\n",
    "N_SIDE_NEIGHBOURS = CONTEXT_WINDOW_SIZE / 2\n",
    "\n",
    "for line in lines:\n",
    "    words = line.split()\n",
    "    for i, word in enumerate(words):\n",
    "        index_for_word_i = word_to_ind[word]\n",
    "        for shift_index in range(-N_SIDE_NEIGHBOURS, N_SIDE_NEIGHBOURS+1):\n",
    "            j = i + shift_index\n",
    "            if (j > 0) and (j < len(words)) and j != i:\n",
    "                word_j = words[j]\n",
    "                index_for_word_j = word_to_ind[word_j]\n",
    "                cooc_matrix[index_for_word_i, index_for_word_j] += 1\n",
    "                cooc_matrix[index_for_word_j, index_for_word_i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9j-LQzLfbdz"
   },
   "source": [
    "Теперь мы можем построить простые векторные представления слов: воспользуемся усечённым SVD-разложением и понизим размерность матрицы совстречаемости слов до 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvASycmqIRoj"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tg3O8UUROtNM"
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=0)\n",
    "svd_embeddings = svd.fit_transform(cooc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlGnx396o5rC"
   },
   "source": [
    "Постройте матрицу попарных косинусных близостей между словами, чтобы искать «соседей» по смыслу. Напомним, что косинусная близость задаётся формулой $\\frac{w_i^T w_j}{||w_i||_2||w_j||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucYA7aN_Po77"
   },
   "outputs": [],
   "source": [
    "# keepdims, чтобы размерность была ок\n",
    "svd_embeddings_normed = svd_embeddings / (np.linalg.norm(svd_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "pairwise_cosine_sim = svd_embeddings_normed @ svd_embeddings_normed.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGlOyQeiiQWI"
   },
   "source": [
    "Выберите произвольное слово из словаря и изучите 10 его ближайших соседей по косинусной близости. Соответствует ли результат вашим ожиданиям?\n",
    "\n",
    "Tip: для ускорения поиска можно вместо сортировки использовать [np.argpartition](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bamWjT6BVvyX"
   },
   "outputs": [],
   "source": [
    "for word in ['cat', 'the', 'dog', 'moscow']:\n",
    "    index_for_word = word_to_ind[word]\n",
    "    cosine_similarities = pairwise_cosine_sim[index_for_word]\n",
    "    neighbor_indices =  np.argsort(cosine_similarities)[-10:][::-1]\n",
    "    neighbor_words = [ind_to_word[ind] for ind in neighbor_indices]\n",
    "    print(f'{word} -> {neighbor_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qlfpaQNTwEo"
   },
   "source": [
    "Теперь попробуем обучить на этих же данных модель Word2Vec. Воспользуйтесь классом [Word2Vec](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.Word2Vec) из библиотеки gensim и обучите модель с размерностью векторов 300, min_count=1; остальные гиперпараметры можно оставить стандартными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bqYo07YWKDI"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences=[line.split() for line in lines], size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rrEDgcsiouf"
   },
   "source": [
    "Проверим качество полученной модели всем известным способом: попробуем найти наиболее близкий вектор к результату арифметической операции king-man+woman. Если вы получили не совсем то, что ожидали, то в чём могут заключаться причины этого?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD93gbgSgbPg"
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar_cosmul(positive=['king','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0af8RenPViwq"
   },
   "source": [
    "Помимо обучения моделей мы можем [загрузить готовые](https://radimrehurek.com/gensim_3.8.3/downloader.html) посредством той же библиотеки Gensim. Загрузим [GloVe](https://nlp.stanford.edu/projects/glove/)-векторы и посмотрим, насколько хорошо с их помощью получается искать аналогии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t0QORHRZj-j-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ph1xTorkhrSW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonina.goryacheva/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('longest', 0.9440028667449951),\n",
       " ('shortest', 0.7883565425872803),\n",
       " ('time', 0.7851566076278687),\n",
       " ('since', 0.77671879529953),\n",
       " ('decades', 0.7765405774116516),\n",
       " ('world', 0.7761632204055786),\n",
       " ('ever', 0.7749701142311096),\n",
       " ('decade', 0.7667874097824097),\n",
       " ('country', 0.7660934329032898),\n",
       " ('biggest', 0.7642965912818909)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.wv.most_similar_cosmul(positive=['tallest', 'long'],negative=['tall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GvdsH9Yjjytr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonina.goryacheva/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('moscow', 0.9874311089515686),\n",
       " ('russian', 0.8762803673744202),\n",
       " ('kremlin', 0.8266472816467285),\n",
       " ('helsinki', 0.8232425451278687),\n",
       " ('petersburg', 0.8097580075263977),\n",
       " ('kiev', 0.7989339828491211),\n",
       " ('putin', 0.7961435914039612),\n",
       " ('tbilisi', 0.7937439680099487),\n",
       " ('interfax', 0.7934480905532837),\n",
       " ('yeltsin', 0.790027379989624)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.wv.most_similar_cosmul(positive=['paris', 'russia'],negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1e_bpYdznqFz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonina.goryacheva/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9199351072311401),\n",
       " ('princess', 0.8403170108795166),\n",
       " ('throne', 0.8287888765335083),\n",
       " ('monarch', 0.8201609253883362),\n",
       " ('elizabeth', 0.8025429248809814),\n",
       " ('daughter', 0.7933654189109802),\n",
       " ('mother', 0.7825508117675781),\n",
       " ('kalākaua', 0.7787636518478394),\n",
       " ('kingdom', 0.777129590511322),\n",
       " ('wife', 0.7694059610366821)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.wv.most_similar_cosmul(positive=['king', 'woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k-meArgjEzo"
   },
   "source": [
    "Визуализируем представления 200 самых частых слов в нашем датасете посредством двух методов понижения размерности — PCA и t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sQnhdDGMpcu7"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roRMI6gLqRbV"
   },
   "outputs": [],
   "source": [
    "most_common_words = [word for word, freq in vocab.most_common(200) if word in glove]\n",
    "common_word_embeddings = np.stack([glove[word] for word in most_common_words if word in glove], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dSsGdAiqrub"
   },
   "outputs": [],
   "source": [
    "pca_representations = PCA(n_components=2,random_state=0).fit_transform(common_word_embeddings)\n",
    "tsne_representations = TSNE(n_components=2,perplexity=25,random_state=0).fit_transform(common_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtNJMOmfq6AH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB4nvzY3q90B"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(pca_representations[:,0],pca_representations[:,1])\n",
    "\n",
    "for i, word in enumerate(most_common_words):\n",
    "    plt.text(pca_representations[i,0]+0.05,pca_representations[i,1]+0.05,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvxsbo0NrIBP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(tsne_representations[:,0],tsne_representations[:,1])\n",
    "\n",
    "for i, word in enumerate(most_common_words):\n",
    "    plt.text(tsne_representations[i,0]+0.05,tsne_representations[i,1]+0.05,word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAydQnYgjOq-"
   },
   "source": [
    "Далее мы воспользуемся библиотекой [fastText](https://github.com/facebookresearch/fastText) от Facebook Research. Она интересна тем, что позволяет очень быстро обучать векторы слов посредством CLI-утилиты, а полученные векторы за счёт суммирования по n-граммам получаются устойчивы к опечаткам или другим небольшим изменениям слов. \n",
    "\n",
    "Установим эту библиотеку (если у вас её ещё нет), а также загрузим предобученную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ8vhhdD1B3G"
   },
   "outputs": [],
   "source": [
    "!pip install -q fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PtrASuqwIsw"
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltFKqNeij3q_"
   },
   "source": [
    "Убедимся, что модель устойчива к опечаткам: попробуйте взять англоязычное слово и посмотрите, есть ли для него вектор в модели, даже если совершить несколько орфографических ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6u-WMHL8SYK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjLu5YJj4T0"
   },
   "source": [
    "Теперь попробуем решить задачу анализа тональности отзывов на фильмы на датасете IMDb. Код ниже скачивает данные и загружает их в pandas.DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Sk1llrqutGL"
   },
   "outputs": [],
   "source": [
    "!wget -q -nc https://github.com/LawrenceDuan/IMDb-Review-Analysis/blob/master/IMDb_Reviews.csv?raw=true -O reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvTdPg2vvXLO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "reviews=pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBG1hgb8vidw"
   },
   "outputs": [],
   "source": [
    "reviews_train, reviews_test=train_test_split(reviews,test_size=1000,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFUGu6Ulmgwf"
   },
   "source": [
    "Постройте матрицы X_train, X_test, Y_train, Y_test, содержащие усреднённые fastText-эмбеддинги слов каждого обзора и метки классов, соответствующие окраске обзора (столбец sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEWnTtryvlNw"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_enc=LabelEncoder()\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUCBoOvRj7Er"
   },
   "source": [
    "В завершение обучим модель логистической регрессии на векторах отзывов. Как видим, даже довольно простой классификатор, не учитывающий порядок слов, может работать довольно неплохо за счёт информативных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntROheDAFvqe"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "model=LogisticRegression(max_iter=500).fit(X_train,Y_train)\n",
    "accuracy_score(model.predict(X_test),Y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "embeddings_sem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
